{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A quick tour of StatsModels.jl\n",
    "### Dave Kleinschmidt\n",
    "### 18 February 2020\n",
    "# Contrast coding\n",
    "\n",
    "## What and why?\n",
    "\n",
    "To fit any kind of statistical model you need some kind of *numerical\n",
    "representation* of your data. Data often comes in a *table*, a named\n",
    "collection of variables of different types of data. Some of that data is\n",
    "\"continuous\", or basically numeric. But often our data is not numeric\n",
    "(or continuous), but \"categorical\", having a finite number of distinct\n",
    "levels.\n",
    "\n",
    "For instance, let's look at the KB07 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling MixedModels [ff71e718-51f3-5ec2-a782-8ffcbfa3c316]\n",
      "└ @ Base loading.jl:1273\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>subj</th><th>item</th><th>spkr</th><th>prec</th><th>load</th><th>rt_trunc</th><th>rt_raw</th></tr><tr><th></th><th>String</th><th>String</th><th>String</th><th>String</th><th>String</th><th>Int16</th><th>Int16</th></tr></thead><tbody><p>5 rows × 7 columns</p><tr><th>1</th><td>S030</td><td>I01</td><td>new</td><td>break</td><td>yes</td><td>2267</td><td>2267</td></tr><tr><th>2</th><td>S030</td><td>I02</td><td>old</td><td>maintain</td><td>no</td><td>3856</td><td>3856</td></tr><tr><th>3</th><td>S030</td><td>I03</td><td>old</td><td>break</td><td>no</td><td>1567</td><td>1567</td></tr><tr><th>4</th><td>S030</td><td>I04</td><td>new</td><td>maintain</td><td>no</td><td>1732</td><td>1732</td></tr><tr><th>5</th><td>S030</td><td>I05</td><td>new</td><td>break</td><td>no</td><td>2660</td><td>2660</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& subj & item & spkr & prec & load & rt\\_trunc & rt\\_raw\\\\\n",
       "\t\\hline\n",
       "\t& String & String & String & String & String & Int16 & Int16\\\\\n",
       "\t\\hline\n",
       "\t1 & S030 & I01 & new & break & yes & 2267 & 2267 \\\\\n",
       "\t2 & S030 & I02 & old & maintain & no & 3856 & 3856 \\\\\n",
       "\t3 & S030 & I03 & old & break & no & 1567 & 1567 \\\\\n",
       "\t4 & S030 & I04 & new & maintain & no & 1732 & 1732 \\\\\n",
       "\t5 & S030 & I05 & new & break & no & 2660 & 2660 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "5×7 DataFrame\n",
       "│ Row │ subj   │ item   │ spkr   │ prec     │ load   │ rt_trunc │ rt_raw │\n",
       "│     │ \u001b[90mString\u001b[39m │ \u001b[90mString\u001b[39m │ \u001b[90mString\u001b[39m │ \u001b[90mString\u001b[39m   │ \u001b[90mString\u001b[39m │ \u001b[90mInt16\u001b[39m    │ \u001b[90mInt16\u001b[39m  │\n",
       "├─────┼────────┼────────┼────────┼──────────┼────────┼──────────┼────────┤\n",
       "│ 1   │ S030   │ I01    │ new    │ break    │ yes    │ 2267     │ 2267   │\n",
       "│ 2   │ S030   │ I02    │ old    │ maintain │ no     │ 3856     │ 3856   │\n",
       "│ 3   │ S030   │ I03    │ old    │ break    │ no     │ 1567     │ 1567   │\n",
       "│ 4   │ S030   │ I04    │ new    │ maintain │ no     │ 1732     │ 1732   │\n",
       "│ 5   │ S030   │ I05    │ new    │ break    │ no     │ 2660     │ 2660   │"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DataFrames, DataFramesMeta\n",
    "using MixedModels, GLM\n",
    "using LinearAlgebra, Statistics\n",
    "\n",
    "kb07 = MixedModels.dataset(:kb07)\n",
    "first(kb07, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `:spkr`, `:prec`, and `:load` are categortical variables, each of\n",
    "which takes on two different values. If we fit a regression using this\n",
    "dataset, we end up with predictors that refer to specific levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear mixed model fit by maximum likelihood\n",
       " rt_trunc ~ 1 + spkr + prec + spkr & prec + (1 | subj)\n",
       "     logLik        -2 logLik          AIC             BIC       \n",
       " -1.45767208×10⁴  2.91534417×10⁴  2.91654417×10⁴  2.91983781×10⁴\n",
       "\n",
       "Variance components:\n",
       "            Column    Variance  Std.Dev. \n",
       "subj     (Intercept)   95885.39 309.65366\n",
       "Residual              662657.47 814.03776\n",
       " Number of obs: 1789; levels of grouping factors: 56\n",
       "\n",
       "  Fixed-effects parameters:\n",
       "──────────────────────────────────────────────────────────────────\n",
       "                             Estimate  Std.Error  z value  P(>|z|)\n",
       "──────────────────────────────────────────────────────────────────\n",
       "(Intercept)                 2425.32      56.5224    42.91   <1e-99\n",
       "spkr: old                    179.992     54.4214     3.31   0.0009\n",
       "prec: maintain              -623.347     54.4214   -11.45   <1e-29\n",
       "spkr: old & prec: maintain   -86.7763    76.9856    -1.13   0.2597\n",
       "──────────────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = @formula(rt_trunc ~ 1 + spkr + prec + spkr&prec + (1 | subj))\n",
    "mod = fit(MixedModel, f, kb07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few rows of the fixed effects design matrix that's\n",
    "generated for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×4 Array{Int64,2}:\n",
       " 1  0  0  0\n",
       " 1  1  1  1\n",
       " 1  1  0  0\n",
       " 1  0  1  0\n",
       " 1  0  0  0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Int.(mod.X)[1:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note: all the values are 0 or 1, and there's one column\n",
    "of all 1s at the start (that's the `(Intercept)` term). Columns 2 and 3\n",
    "correspond to `spkr` and `prec`: there's a 0 where `spkr == \"new\"` and a\n",
    "1 for `\"old\"`. Note that the coefficient name for this column is `spkr:\n",
    "old`, which indicates that this predictor indicates the presence of\n",
    "\"old\", relative to the (implicit) baseline of \"new\". Similarly for\n",
    "`prec: maintain`.\n",
    "\n",
    "The last column is the interaction term `spkr&prec`, and it's the\n",
    "elementwise product of the columns for `spkr: new` and `pred: maintain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>subj</th><th>item</th><th>spkr</th><th>prec</th><th>load</th><th>rt_trunc</th><th>rt_raw</th></tr><tr><th></th><th>String</th><th>String</th><th>String</th><th>String</th><th>String</th><th>Int16</th><th>Int16</th></tr></thead><tbody><p>5 rows × 7 columns</p><tr><th>1</th><td>S030</td><td>I01</td><td>new</td><td>break</td><td>yes</td><td>2267</td><td>2267</td></tr><tr><th>2</th><td>S030</td><td>I02</td><td>old</td><td>maintain</td><td>no</td><td>3856</td><td>3856</td></tr><tr><th>3</th><td>S030</td><td>I03</td><td>old</td><td>break</td><td>no</td><td>1567</td><td>1567</td></tr><tr><th>4</th><td>S030</td><td>I04</td><td>new</td><td>maintain</td><td>no</td><td>1732</td><td>1732</td></tr><tr><th>5</th><td>S030</td><td>I05</td><td>new</td><td>break</td><td>no</td><td>2660</td><td>2660</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& subj & item & spkr & prec & load & rt\\_trunc & rt\\_raw\\\\\n",
       "\t\\hline\n",
       "\t& String & String & String & String & String & Int16 & Int16\\\\\n",
       "\t\\hline\n",
       "\t1 & S030 & I01 & new & break & yes & 2267 & 2267 \\\\\n",
       "\t2 & S030 & I02 & old & maintain & no & 3856 & 3856 \\\\\n",
       "\t3 & S030 & I03 & old & break & no & 1567 & 1567 \\\\\n",
       "\t4 & S030 & I04 & new & maintain & no & 1732 & 1732 \\\\\n",
       "\t5 & S030 & I05 & new & break & no & 2660 & 2660 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "5×7 DataFrame\n",
       "│ Row │ subj   │ item   │ spkr   │ prec     │ load   │ rt_trunc │ rt_raw │\n",
       "│     │ \u001b[90mString\u001b[39m │ \u001b[90mString\u001b[39m │ \u001b[90mString\u001b[39m │ \u001b[90mString\u001b[39m   │ \u001b[90mString\u001b[39m │ \u001b[90mInt16\u001b[39m    │ \u001b[90mInt16\u001b[39m  │\n",
       "├─────┼────────┼────────┼────────┼──────────┼────────┼──────────┼────────┤\n",
       "│ 1   │ S030   │ I01    │ new    │ break    │ yes    │ 2267     │ 2267   │\n",
       "│ 2   │ S030   │ I02    │ old    │ maintain │ no     │ 3856     │ 3856   │\n",
       "│ 3   │ S030   │ I03    │ old    │ break    │ no     │ 1567     │ 1567   │\n",
       "│ 4   │ S030   │ I04    │ new    │ maintain │ no     │ 1732     │ 1732   │\n",
       "│ 5   │ S030   │ I05    │ new    │ break    │ no     │ 2660     │ 2660   │"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb07[1:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to take control\n",
    "\n",
    "You can set your own contrasts via the `contrasts=` keyword argument in\n",
    "`fit`, with the variable you want to code as the key and contrasts as\n",
    "the value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear mixed model fit by maximum likelihood\n",
       " rt_trunc ~ 1 + spkr + prec + spkr & prec + (1 | subj)\n",
       "     logLik        -2 logLik          AIC             BIC       \n",
       " -1.45767208×10⁴  2.91534417×10⁴  2.91654417×10⁴  2.91983781×10⁴\n",
       "\n",
       "Variance components:\n",
       "            Column    Variance  Std.Dev. \n",
       "subj     (Intercept)   95885.39 309.65366\n",
       "Residual              662657.47 814.03776\n",
       " Number of obs: 1789; levels of grouping factors: 56\n",
       "\n",
       "  Fixed-effects parameters:\n",
       "───────────────────────────────────────────────────────────────\n",
       "                          Estimate  Std.Error  z value  P(>|z|)\n",
       "───────────────────────────────────────────────────────────────\n",
       "(Intercept)              1848.58      49.5329    37.32   <1e-99\n",
       "spkr: new                 -46.6081    27.2263    -1.71   0.0869\n",
       "prec: break               666.735     38.4928    17.32   <1e-66\n",
       "spkr: new & prec: break   -43.3882    38.4928    -1.13   0.2597\n",
       "───────────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using StatsModels\n",
    "\n",
    "contrasts = Dict(\n",
    "    :spkr => EffectsCoding(base = \"old\"),\n",
    "    :prec => DummyCoding(levels = [\"maintain\", \"break\"])\n",
    ")\n",
    "\n",
    "mod2 = fit(MixedModel, f, kb07, contrasts=contrasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example illustrates two ways to control the ordering of levels used\n",
    "to compute the contrasts:\n",
    "\n",
    "1.  you can use `base=` to determine the baseline level\n",
    "2.  you can use `levels=` to indicate all the levels that are used in\n",
    "    the contrasts, the first of which is automatically set as the\n",
    "    baseline.\n",
    "\n",
    "### Reversed Helmert coding\n",
    "\n",
    "Let's say you want to use reverse Helmert coding.  It's easy using `reverse` to\n",
    "flip the order of the levels.  Here's the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear mixed model fit by maximum likelihood\n",
       " rt_trunc ~ 1 + spkr + prec + spkr & prec + (1 | subj)\n",
       "     logLik        -2 logLik          AIC             BIC       \n",
       " -1.45767208×10⁴  2.91534417×10⁴  2.91654417×10⁴  2.91983781×10⁴\n",
       "\n",
       "Variance components:\n",
       "            Column    Variance  Std.Dev. \n",
       "subj     (Intercept)   95885.39 309.65366\n",
       "Residual              662657.47 814.03776\n",
       " Number of obs: 1789; levels of grouping factors: 56\n",
       "\n",
       "  Fixed-effects parameters:\n",
       "──────────────────────────────────────────────────────────────────\n",
       "                             Estimate  Std.Error  z value  P(>|z|)\n",
       "──────────────────────────────────────────────────────────────────\n",
       "(Intercept)                 2515.31      49.5243    50.79   <1e-99\n",
       "spkr: new                    -89.9962    27.2107    -3.31   0.0009\n",
       "prec: maintain              -666.735     38.4928   -17.32   <1e-66\n",
       "spkr: new & prec: maintain    43.3882    38.4928     1.13   0.2597\n",
       "──────────────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spkr_levels = [\"old\",\"new\"]\n",
    "fit(MixedModel,\n",
    "    f,\n",
    "    kb07,\n",
    "    contrasts = Dict(:spkr => HelmertCoding(levels=spkr_levels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And reversed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear mixed model fit by maximum likelihood\n",
       " rt_trunc ~ 1 + spkr + prec + spkr & prec + (1 | subj)\n",
       "     logLik        -2 logLik          AIC             BIC       \n",
       " -1.45767208×10⁴  2.91534417×10⁴  2.91654417×10⁴  2.91983781×10⁴\n",
       "\n",
       "Variance components:\n",
       "            Column    Variance  Std.Dev. \n",
       "subj     (Intercept)   95885.39 309.65366\n",
       "Residual              662657.47 814.03776\n",
       " Number of obs: 1789; levels of grouping factors: 56\n",
       "\n",
       "  Fixed-effects parameters:\n",
       "──────────────────────────────────────────────────────────────────\n",
       "                             Estimate  Std.Error  z value  P(>|z|)\n",
       "──────────────────────────────────────────────────────────────────\n",
       "(Intercept)                 2515.31      49.5243    50.79   <1e-99\n",
       "spkr: old                     89.9962    27.2107     3.31   0.0009\n",
       "prec: maintain              -666.735     38.4928   -17.32   <1e-66\n",
       "spkr: old & prec: maintain   -43.3882    38.4928    -1.13   0.2597\n",
       "──────────────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(MixedModel,\n",
    "    f,\n",
    "    kb07,\n",
    "    contrasts = Dict(:spkr => HelmertCoding(levels=reverse(spkr_levels))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built in contrast coding schemes\n",
    "\n",
    "StatsModels.jl provides a few commonly used contrast coding schemes,\n",
    "some less-commonly used schemes, and structs that allow you to manually\n",
    "specify your own, custom schemes.\n",
    "\n",
    "All are subtypes of the `AbstractContrasts` type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element Array{Any,1}:\n",
       " ContrastsCoding            \n",
       " DummyCoding                \n",
       " EffectsCoding              \n",
       " HelmertCoding              \n",
       " HypothesisCoding           \n",
       " SeqDiffCoding              \n",
       " StatsModels.FullDummyCoding"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using InteractiveUtils\n",
    "subtypes(AbstractContrasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And all have fairly extensive documentation via the normal help system.\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ?SeqDiffCoding in the REPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard contrasts\n",
    "\n",
    "The most commonly used contrasts are `DummyCoding` and `EffectsCoding`\n",
    "(which are similar to `contr.treatment` and `contr.sum` in R,\n",
    "respectively).\n",
    "\n",
    "### \"Exotic\" contrasts\n",
    "\n",
    "We also provide `HelmertCoding` and `SeqDiffCoding` (corresponding to\n",
    "base R's `contr.helmert` and MASS's `contr.sdiff`).\n",
    "\n",
    "## Manual contrasts\n",
    "\n",
    "There are two ways to manually specify contrasts. First, you can specify\n",
    "them **directly** via `ContrastsCoding`. If you do, it's good practice\n",
    "to specify the levels corresponding to the rows of the matrix, although\n",
    "they can be omitted in which case they'll be inferred from the data.\n",
    "\n",
    "For instance, here's a weird set of contrasts for `:spkr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `ContrastsCoding(contrasts)` is deprecated and will not be exported in the future, use `HypothesisCoding(pinv(contrasts))` instead.\n",
      "│   caller = #ContrastsCoding#8 at contrasts.jl:526 [inlined]\n",
      "└ @ Core /home/dave/.julia/packages/StatsModels/NakzS/src/contrasts.jl:526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear mixed model fit by maximum likelihood\n",
       " rt_trunc ~ 1 + spkr + prec + spkr & prec + (1 | subj)\n",
       "     logLik        -2 logLik          AIC             BIC       \n",
       " -1.45767208×10⁴  2.91534417×10⁴  2.91654417×10⁴  2.91983781×10⁴\n",
       "\n",
       "Variance components:\n",
       "            Column    Variance  Std.Dev. \n",
       "subj     (Intercept)   95885.39 309.65366\n",
       "Residual              662657.47 814.03776\n",
       " Number of obs: 1789; levels of grouping factors: 56\n",
       "\n",
       "  Fixed-effects parameters:\n",
       "──────────────────────────────────────────────────────────────────\n",
       "                             Estimate  Std.Error  z value  P(>|z|)\n",
       "──────────────────────────────────────────────────────────────────\n",
       "(Intercept)                 2545.31      50.3425    50.56   <1e-99\n",
       "spkr: new                   -179.992     54.4214    -3.31   0.0009\n",
       "prec: maintain              -681.198     40.582    -16.79   <1e-62\n",
       "spkr: new & prec: maintain    86.7763    76.9856     1.13   0.2597\n",
       "──────────────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs = Matrix([-1/3 2/3]')\n",
    "contr_manual = Dict(:spkr => StatsModels.ContrastsCoding(cs, levels=[\"old\", \"new\"]))\n",
    "mod3 = fit(MixedModel, f, kb07, contrasts=contr_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that the estimates and even the signs of the fixed effect βs\n",
    "change when we change the contrasts, but the overall log-likelihood\n",
    "doesn't).\n",
    "\n",
    "We can see that the values from the contrasts matrix we specified are\n",
    "plugged directly in to the fixed effects matrix, and are also used in\n",
    "computing the predictor for the interaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×4 Array{Float64,2}:\n",
       " 1.0   0.666667  0.0   0.0     \n",
       " 1.0  -0.333333  1.0  -0.333333\n",
       " 1.0  -0.333333  0.0  -0.0     \n",
       " 1.0   0.666667  1.0   0.666667\n",
       " 1.0   0.666667  0.0   0.0     "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod3.X[1:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: manual Helmert contrasts\n",
    "\n",
    "Let's say you want Helmert contrasts but you always forget what it's called.\n",
    "Here's how you can manually specify them using `StatsModels.ContrastsCoding`.\n",
    "\n",
    "Because this isn't very interesting with only two levels, let's combine `:spkr`\n",
    "and `:prec` into a single, 4-level variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FormulaTerm\n",
       "Response:\n",
       "  rt_trunc(unknown)\n",
       "Predictors:\n",
       "  1\n",
       "  spkr_prec(unknown)\n",
       "  (subj)->1 | subj"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb07ex = @transform(kb07, spkr_prec = :spkr .* \"-\" .* :prec);\n",
    "levels = [\"new-break\", \"new-maintain\", \"old-break\", \"old-maintain\"]\n",
    "f2 = @formula(rt_trunc ~ 1 + spkr_prec + (1 | subj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `ContrastsCoding(contrasts)` is deprecated and will not be exported in the future, use `HypothesisCoding(pinv(contrasts))` instead.\n",
      "│   caller = #ContrastsCoding#8 at contrasts.jl:526 [inlined]\n",
      "└ @ Core /home/dave/.julia/packages/StatsModels/NakzS/src/contrasts.jl:526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear mixed model fit by maximum likelihood\n",
       " rt_trunc ~ 1 + spkr_prec + (1 | subj)\n",
       "     logLik        -2 logLik          AIC             BIC       \n",
       " -1.45767208×10⁴  2.91534417×10⁴  2.91654417×10⁴  2.91983781×10⁴\n",
       "\n",
       "Variance components:\n",
       "            Column    Variance  Std.Dev. \n",
       "subj     (Intercept)   95885.39 309.65366\n",
       "Residual              662657.47 814.03776\n",
       " Number of obs: 1789; levels of grouping factors: 56\n",
       "\n",
       "  Fixed-effects parameters:\n",
       "───────────────────────────────────────────────────────────────\n",
       "                          Estimate  Std.Error  z value  P(>|z|)\n",
       "───────────────────────────────────────────────────────────────\n",
       "(Intercept)              2181.94      45.6362    47.81   <1e-99\n",
       "spkr_prec: new-maintain  -311.673     27.2107   -11.45   <1e-29\n",
       "spkr_prec: old-break      163.889     15.7041    10.44   <1e-24\n",
       "spkr_prec: old-maintain   -95.5865    11.1225    -8.59   <1e-17\n",
       "───────────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using StatsModels: ContrastsCoding\n",
    "man_helm = [-1 -1 -1\n",
    "             1 -1 -1\n",
    "             0  2 -1\n",
    "             0  0  3]\n",
    "contr_helm_man = ContrastsCoding(man_helm[:,1:3], levels=levels)\n",
    "fit(MixedModel, f2, kb07ex, contrasts = Dict(:spkr_prec => contr_helm_man))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is equivalent to `HelmertCoding`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear mixed model fit by maximum likelihood\n",
       " rt_trunc ~ 1 + spkr_prec + (1 | subj)\n",
       "     logLik        -2 logLik          AIC             BIC       \n",
       " -1.45767208×10⁴  2.91534417×10⁴  2.91654417×10⁴  2.91983781×10⁴\n",
       "\n",
       "Variance components:\n",
       "            Column    Variance  Std.Dev. \n",
       "subj     (Intercept)   95885.39 309.65366\n",
       "Residual              662657.47 814.03776\n",
       " Number of obs: 1789; levels of grouping factors: 56\n",
       "\n",
       "  Fixed-effects parameters:\n",
       "───────────────────────────────────────────────────────────────\n",
       "                          Estimate  Std.Error  z value  P(>|z|)\n",
       "───────────────────────────────────────────────────────────────\n",
       "(Intercept)              2181.94      45.6362    47.81   <1e-99\n",
       "spkr_prec: new-maintain  -311.673     27.2107   -11.45   <1e-29\n",
       "spkr_prec: old-break      163.889     15.7041    10.44   <1e-24\n",
       "spkr_prec: old-maintain   -95.5865    11.1225    -8.59   <1e-17\n",
       "───────────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(MixedModel, f2, kb07ex, contrasts = Dict(:spkr_prec => HelmertCoding(levels=levels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrasts from hypotheses\n",
    "\n",
    "A better way to specify manual contrasts is via `HypothesisCoding`,\n",
    "where each row of the matrix corresponds to the weights given to the\n",
    "cell means of the levels corresponding to each column (see [Schad et\n",
    "al. 2020](https://doi.org/10.1016/j.jml.2019.104038) for more\n",
    "information). This is less interesting with only two levels, so let's\n",
    "look at a scenario where we combine `:spkr` and `:prec` into a single,\n",
    "4-level predictor, and want to test some strange hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear mixed model fit by maximum likelihood\n",
       " rt_trunc ~ 1 + spkr_prec + (1 | subj)\n",
       "     logLik        -2 logLik          AIC             BIC       \n",
       " -1.45767208×10⁴  2.91534417×10⁴  2.91654417×10⁴  2.91983781×10⁴\n",
       "\n",
       "Variance components:\n",
       "            Column    Variance  Std.Dev. \n",
       "subj     (Intercept)   95885.39 309.65366\n",
       "Residual              662657.47 814.03776\n",
       " Number of obs: 1789; levels of grouping factors: 56\n",
       "\n",
       "  Fixed-effects parameters:\n",
       "──────────────────────────────────────────────────────────────\n",
       "                         Estimate  Std.Error  z value  P(>|z|)\n",
       "──────────────────────────────────────────────────────────────\n",
       "(Intercept)              2425.32     56.5224    42.91   <1e-99\n",
       "spkr_prec: new-maintain  -623.347    54.4214   -11.45   <1e-29\n",
       "spkr_prec: old-break      179.992    54.4214     3.31   0.0009\n",
       "spkr_prec: old-maintain  -530.131    54.4839    -9.73   <1e-21\n",
       "──────────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2 = @formula(rt_trunc ~ 1 + spkr_prec + (1 | subj))\n",
    "mod4 = fit(MixedModel, f2, kb07ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to test whether the effect of `:prec` depends on\n",
    "whether `:spkr` is old vs. new. We need one contrast to test the\n",
    "hypothesis that `\"maintain\" != \"break\"` for \"new\", and another for\n",
    "\"old\". That leaves one over, to test the overall difference between\n",
    "\"new\" and \"old\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Int64,1}:\n",
       "  0\n",
       "  0\n",
       "  1\n",
       " -1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_old = (levels .== \"old-break\") .- (levels .== \"old-maintain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Int64,1}:\n",
       "  1\n",
       " -1\n",
       "  0\n",
       "  0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_new = (levels .== \"new-break\") .- (levels .== \"new-maintain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " -0.5\n",
       " -0.5\n",
       "  0.5\n",
       "  0.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_new = (abs.(prec_old) .- abs.(prec_new)) ./ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×4 Array{Float64,2}:\n",
       " -0.5  -0.5  0.5   0.5\n",
       "  0.0   0.0  1.0  -1.0\n",
       "  1.0  -1.0  0.0   0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contr_hyp = HypothesisCoding(Matrix(hcat(old_new, prec_old, prec_new)'),\n",
    "                             labels=[\"old\", \"(old) break\", \"(new) break\"])\n",
    "contr_hyp.hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These hypotheses correspond to the following *contrasts* (using the\n",
    "[`rationalize`](https://docs.julialang.org/en/v1/base/math/#Base.rationalize)\n",
    "function to make pretty fractions, like the `fractions` function in R):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×3 Array{Rational{Int64},2}:\n",
       " -1//2   0//1   1//2\n",
       " -1//2   0//1  -1//2\n",
       "  1//2   1//2   0//1\n",
       "  1//2  -1//2   0//1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rationalize.(contr_hyp.contrasts, tol=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the ±1 coding in the hypotheses (which translates into the difference\n",
    "between the mean response in those cells) is transformed into ±½ coding in the\n",
    "contrasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear mixed model fit by maximum likelihood\n",
       " rt_trunc ~ 1 + spkr_prec + (1 | subj)\n",
       "     logLik        -2 logLik          AIC             BIC       \n",
       " -1.45767208×10⁴  2.91534417×10⁴  2.91654417×10⁴  2.91983781×10⁴\n",
       "\n",
       "Variance components:\n",
       "            Column    Variance  Std.Dev. \n",
       "subj     (Intercept)   95885.39 309.65366\n",
       "Residual              662657.47 814.03776\n",
       " Number of obs: 1789; levels of grouping factors: 56\n",
       "\n",
       "  Fixed-effects parameters:\n",
       "─────────────────────────────────────────────────────────────\n",
       "                        Estimate  Std.Error  z value  P(>|z|)\n",
       "─────────────────────────────────────────────────────────────\n",
       "(Intercept)             2181.94     45.6362    47.81   <1e-99\n",
       "spkr_prec: old           136.604    38.4928     3.55   0.0004\n",
       "spkr_prec: (old) break   710.123    54.4527    13.04   <1e-38\n",
       "spkr_prec: (new) break   623.347    54.4214    11.45   <1e-29\n",
       "─────────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod5 = fit(MixedModel, f2, kb07ex, contrasts = Dict(:spkr_prec => contr_hyp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is equivalent to the `/` \"nesting\" syntax using `EffectsCoding`,\n",
    "after adjusting for the 2× factor from the +1/-1 coding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear mixed model fit by maximum likelihood\n",
       " rt_trunc ~ 1 + spkr + spkr & prec + (1 | subj)\n",
       "     logLik        -2 logLik          AIC             BIC       \n",
       " -1.45767208×10⁴  2.91534417×10⁴  2.91654417×10⁴  2.91983781×10⁴\n",
       "\n",
       "Variance components:\n",
       "            Column    Variance  Std.Dev. \n",
       "subj     (Intercept)   95885.39 309.65366\n",
       "Residual              662657.47 814.03776\n",
       " Number of obs: 1789; levels of grouping factors: 56\n",
       "\n",
       "  Fixed-effects parameters:\n",
       "───────────────────────────────────────────────────────────────\n",
       "                          Estimate  Std.Error  z value  P(>|z|)\n",
       "───────────────────────────────────────────────────────────────\n",
       "(Intercept)              2181.94      45.6362    47.81   <1e-99\n",
       "spkr: old                  68.3021    19.2464     3.55   0.0004\n",
       "spkr: new & prec: break   311.673     27.2107    11.45   <1e-29\n",
       "spkr: old & prec: break   355.062     27.2263    13.04   <1e-38\n",
       "───────────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod6 = fit(MixedModel,\n",
    "           @formula(rt_trunc ~ 1 + spkr/prec + (1|subj)), \n",
    "           kb07,\n",
    "           contrasts = Dict(:spkr => EffectsCoding(base=\"new\"),\n",
    "                            :prec => EffectsCoding(base=\"maintain\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Helmert contrasts that actually make sense\n",
    "\n",
    "Let's say you want something like Helmert contrasts, but where the βs are\n",
    "interpretable as the difference between the $n$th level and the average of\n",
    "levels $1\\ldots n-1$.  Here are the hypotheses that correspond to that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×3 Array{Float64,2}:\n",
       " -1.0  -0.5  -0.333333\n",
       "  1.0  -0.5  -0.333333\n",
       "  0.0   1.0  -0.333333\n",
       "  0.0   0.0   1.0     "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helmert_hypotheses = [-1 -1/2 -1/3\n",
    "                       1 -1/2 -1/3\n",
    "                       0  1   -1/3\n",
    "                       0  0    1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the resulting contrasts matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×3 Array{Float64,2}:\n",
       " -0.5          -0.333333     -0.25\n",
       "  0.5          -0.333333     -0.25\n",
       "  5.88785e-17   0.666667     -0.25\n",
       "  5.88785e-17   2.97183e-16   0.75"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contr_helm_hyp = HypothesisCoding(Matrix(helmert_hypotheses'),\n",
    "                                  levels=levels, labels=levels[2:end])\n",
    "contr_helm_hyp.contrasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is similar but not identical to the contrats matrix for HelmertCoding!  In\n",
    "a way that I would not be able to derive off the top of my head.\n",
    "\n",
    "Now we fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear mixed model fit by maximum likelihood\n",
       " rt_trunc ~ 1 + spkr_prec + (1 | subj)\n",
       "     logLik        -2 logLik          AIC             BIC       \n",
       " -1.45767208×10⁴  2.91534417×10⁴  2.91654417×10⁴  2.91983781×10⁴\n",
       "\n",
       "Variance components:\n",
       "            Column    Variance  Std.Dev. \n",
       "subj     (Intercept)   95885.39 309.65366\n",
       "Residual              662657.47 814.03776\n",
       " Number of obs: 1789; levels of grouping factors: 56\n",
       "\n",
       "  Fixed-effects parameters:\n",
       "──────────────────────────────────────────────────────────────\n",
       "                         Estimate  Std.Error  z value  P(>|z|)\n",
       "──────────────────────────────────────────────────────────────\n",
       "(Intercept)              2181.94     45.6362    47.81   <1e-99\n",
       "spkr_prec: new-maintain  -623.347    54.4214   -11.45   <1e-29\n",
       "spkr_prec: old-break      491.666    47.1123    10.44   <1e-24\n",
       "spkr_prec: old-maintain  -382.346    44.4902    -8.59   <1e-17\n",
       "──────────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(MixedModel, f2, kb07ex, contrasts = Dict(:spkr_prec => contr_helm_hyp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and we can see that the βs for levels are very close to the cumulative means\n",
    "minus the mean for that level, computed manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>spkr_prec</th><th>mean_rt</th><th>cum_mean</th><th>diff_with_last_mean</th></tr><tr><th></th><th>String</th><th>Float64</th><th>Float64</th><th>Float64⍰</th></tr></thead><tbody><p>4 rows × 4 columns</p><tr><th>1</th><td>new-break</td><td>2425.43</td><td>2425.43</td><td>missing</td></tr><tr><th>2</th><td>new-maintain</td><td>1801.97</td><td>2113.7</td><td>-623.465</td></tr><tr><th>3</th><td>old-break</td><td>2605.31</td><td>2277.57</td><td>491.607</td></tr><tr><th>4</th><td>old-maintain</td><td>1895.35</td><td>2182.02</td><td>-382.218</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& spkr\\_prec & mean\\_rt & cum\\_mean & diff\\_with\\_last\\_mean\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64⍰\\\\\n",
       "\t\\hline\n",
       "\t1 & new-break & 2425.43 & 2425.43 &  \\\\\n",
       "\t2 & new-maintain & 1801.97 & 2113.7 & -623.465 \\\\\n",
       "\t3 & old-break & 2605.31 & 2277.57 & 491.607 \\\\\n",
       "\t4 & old-maintain & 1895.35 & 2182.02 & -382.218 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "4×4 DataFrame\n",
       "│ Row │ spkr_prec    │ mean_rt │ cum_mean │ diff_with_last_mean │\n",
       "│     │ \u001b[90mString\u001b[39m       │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m  │ \u001b[90mFloat64⍰\u001b[39m            │\n",
       "├─────┼──────────────┼─────────┼──────────┼─────────────────────┤\n",
       "│ 1   │ new-break    │ 2425.43 │ 2425.43  │ \u001b[90mmissing\u001b[39m             │\n",
       "│ 2   │ new-maintain │ 1801.97 │ 2113.7   │ -623.465            │\n",
       "│ 3   │ old-break    │ 2605.31 │ 2277.57  │ 491.607             │\n",
       "│ 4   │ old-maintain │ 1895.35 │ 2182.02  │ -382.218            │"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (using join to make sure the levels are in the right order)\n",
    "lev_means = join(DataFrame(spkr_prec=levels),\n",
    "                 by(kb07ex, :spkr_prec, mean_rt = :rt_trunc => mean),\n",
    "                 on = :spkr_prec)\n",
    "\n",
    "lev_means[!, :cum_mean] .= cumsum(lev_means.mean_rt) ./ (1:4)\n",
    "lev_means[!, :diff_with_last_mean] .= lev_means.mean_rt .- lag(lev_means.cum_mean)\n",
    "\n",
    "lev_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The `@formula`\n",
    "\n",
    "A formula in Julia is created with the `@formula` macro.  Between the macro and\n",
    "fitting a model, the formula goes through a number of steps.\n",
    "\n",
    "* The `@formula` macro itself does some transformations on the syntax, and\n",
    "  creates `Term`s\n",
    "* Then a `Schema` is extracted from the data, which says which `Term`s are\n",
    "  `ContinuousTerm`s and which are `CategoricalTerm`s\n",
    "* The `Schema` is then used to transform the original formula into a \"concrete\n",
    "  formula\".\n",
    "* The concrete formula (with all `Term`s replaced by continuous/categorical\n",
    "  versions) generates model matrix columns when given some data.\n",
    "\n",
    "The details are described in the\n",
    "[documentation](https://juliastats.org/StatsModels.jl/stable/internals/#The-lifecycle-of-a-@formula-1),\n",
    "and for the most part modeling packages handle these steps for you.  But in the\n",
    "interest of allowing you to do your own weird things, here are a few examples.\n",
    "\n",
    "## A formula is made of terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FormulaTerm\n",
       "Response:\n",
       "  y(unknown)\n",
       "Predictors:\n",
       "  1\n",
       "  a(unknown)\n",
       "  b(unknown)\n",
       "  a(unknown) & b(unknown)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = @formula(y ~ 1 + a + b + a&b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inpsect the internal structure with (it's like `str` in R):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FormulaTerm{Term,Tuple{ConstantTerm{Int64},Term,Term,InteractionTerm{Tuple{Term,Term}}}}\n",
      "  lhs: Term\n",
      "    sym: Symbol y\n",
      "  rhs: Tuple{ConstantTerm{Int64},Term,Term,InteractionTerm{Tuple{Term,Term}}}\n",
      "    1: ConstantTerm{Int64}\n",
      "      n: Int64 1\n",
      "    2: Term\n",
      "      sym: Symbol a\n",
      "    3: Term\n",
      "      sym: Symbol b\n",
      "    4: InteractionTerm{Tuple{Term,Term}}\n",
      "      terms: Tuple{Term,Term}\n",
      "        1: Term\n",
      "          sym: Symbol a\n",
      "        2: Term\n",
      "          sym: Symbol b\n"
     ]
    }
   ],
   "source": [
    "dump(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build the same formula directly, using terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term\n",
      "  sym: Symbol a\n"
     ]
    }
   ],
   "source": [
    "t_a = term(:a)\n",
    "t_b = term(:b)\n",
    "t_1 = term(1)\n",
    "t_y = term(:y)\n",
    "\n",
    "dump(t_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FormulaTerm\n",
       "Response:\n",
       "  y(unknown)\n",
       "Predictors:\n",
       "  1\n",
       "  a(unknown)\n",
       "  b(unknown)\n",
       "  a(unknown) & b(unknown)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_dir = FormulaTerm(t_y, (t_1, t_a, t_b, InteractionTerm((t_a, t_b))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, using operator overloading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FormulaTerm\n",
       "Response:\n",
       "  y(unknown)\n",
       "Predictors:\n",
       "  1\n",
       "  a(unknown)\n",
       "  b(unknown)\n",
       "  a(unknown) & b(unknown)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_op = t_y ~ t_1 + t_a + t_b + t_a & t_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three are all equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f == f_dir == f_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The schema gives concrete terms\n",
    "\n",
    "If we have some fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>y</th><th>a</th><th>b</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Symbol</th></tr></thead><tbody><p>5 rows × 3 columns</p><tr><th>1</th><td>0.471251</td><td>0.650488</td><td>Q</td></tr><tr><th>2</th><td>0.945952</td><td>0.887735</td><td>R</td></tr><tr><th>3</th><td>0.629888</td><td>0.998381</td><td>S</td></tr><tr><th>4</th><td>0.334476</td><td>0.565725</td><td>T</td></tr><tr><th>5</th><td>0.747849</td><td>0.506876</td><td>Q</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& y & a & b\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Symbol\\\\\n",
       "\t\\hline\n",
       "\t1 & 0.471251 & 0.650488 & Q \\\\\n",
       "\t2 & 0.945952 & 0.887735 & R \\\\\n",
       "\t3 & 0.629888 & 0.998381 & S \\\\\n",
       "\t4 & 0.334476 & 0.565725 & T \\\\\n",
       "\t5 & 0.747849 & 0.506876 & Q \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "5×3 DataFrame\n",
       "│ Row │ y        │ a        │ b      │\n",
       "│     │ \u001b[90mFloat64\u001b[39m  │ \u001b[90mFloat64\u001b[39m  │ \u001b[90mSymbol\u001b[39m │\n",
       "├─────┼──────────┼──────────┼────────┤\n",
       "│ 1   │ 0.471251 │ 0.650488 │ Q      │\n",
       "│ 2   │ 0.945952 │ 0.887735 │ R      │\n",
       "│ 3   │ 0.629888 │ 0.998381 │ S      │\n",
       "│ 4   │ 0.334476 │ 0.565725 │ T      │\n",
       "│ 5   │ 0.747849 │ 0.506876 │ Q      │"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = DataFrame(y = rand(100), a = rand(100), b = repeat([:Q, :R, :S, :T], 25))\n",
    "first(df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract a `Schema`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StatsModels.Schema with 3 entries:\n",
       "  a => a\n",
       "  b => b\n",
       "  y => y\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sch = schema(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This maps un-typed `Term`s to concrete verisons.  Now we know that `:a` is a\n",
    "continuous variable in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a(continuous)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sch[term(:a)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical terms hold the contrasts matrix and levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b(DummyCoding:4→3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_b_concrete = sch[term(:b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×3 Array{Float64,2}:\n",
       " 0.0  0.0  0.0\n",
       " 1.0  0.0  0.0\n",
       " 0.0  1.0  0.0\n",
       " 0.0  0.0  1.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmat = t_b_concrete.contrasts\n",
    "cmat.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Symbol,1}:\n",
       " :Q\n",
       " :R\n",
       " :S\n",
       " :T"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmat.levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `apply_schema` combines terms and schema to get concrete versions\n",
    "\n",
    "The canonical case is to apply the schema to the whole formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FormulaTerm\n",
       "Response:\n",
       "  y(continuous)\n",
       "Predictors:\n",
       "  1\n",
       "  a(continuous)\n",
       "  b(DummyCoding:4→3)\n",
       "  a(continuous) & b(DummyCoding:4→3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_schema(f, sch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the schema gets pushed through the interaction term, too.\n",
    "\n",
    "We can also apply the schema to a single term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a(continuous) & b(DummyCoding:4→3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_schema(term(:a) & term(:b), sch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course if the schema doesn't have enough information, we'll get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "KeyError: key argle_bargle not found",
     "output_type": "error",
     "traceback": [
      "KeyError: key argle_bargle not found",
      "",
      "Stacktrace:",
      " [1] getindex at ./dict.jl:477 [inlined]",
      " [2] getindex at /home/dave/.julia/packages/StatsModels/NakzS/src/schema.jl:48 [inlined]",
      " [3] apply_schema at /home/dave/.julia/packages/StatsModels/NakzS/src/schema.jl:212 [inlined]",
      " [4] apply_schema(::Term, ::StatsModels.Schema) at /home/dave/.julia/packages/StatsModels/NakzS/src/schema.jl:208",
      " [5] top-level scope at In[41]:1"
     ]
    }
   ],
   "source": [
    "apply_schema(term(:argle_bargle), sch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concrete terms generate arrays with `modelcols`\n",
    "\n",
    "Any term can generate model columns with `modelcols`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×3 Array{Float64,2}:\n",
       " 0.0       0.0       0.0     \n",
       " 0.887735  0.0       0.0     \n",
       " 0.0       0.998381  0.0     \n",
       " 0.0       0.0       0.565725\n",
       " 0.0       0.0       0.0     \n",
       " 0.587189  0.0       0.0     "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_ab_concrete = apply_schema(term(:a) & term(:b), sch)\n",
    "modelcols(t_ab_concrete, first(df, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_ab_concrete_formula = f_concrete.rhs.terms[end] = a & b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6×3 Array{Float64,2}:\n",
       " 0.0       0.0       0.0     \n",
       " 0.887735  0.0       0.0     \n",
       " 0.0       0.998381  0.0     \n",
       " 0.0       0.0       0.565725\n",
       " 0.0       0.0       0.0     \n",
       " 0.587189  0.0       0.0     "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_concrete = apply_schema(f, sch)\n",
    "@show t_ab_concrete_formula = f_concrete.rhs.terms[end]\n",
    "modelcols(t_ab_concrete_formula, first(df, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course you can generate columns for the whole formula (it returns a tuple of\n",
    "left-hand side, right-hand side columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×8 Array{Float64,2}:\n",
       " 1.0  0.650488  0.0  0.0  0.0  0.0       0.0       0.0     \n",
       " 1.0  0.887735  1.0  0.0  0.0  0.887735  0.0       0.0     \n",
       " 1.0  0.998381  0.0  1.0  0.0  0.0       0.998381  0.0     \n",
       " 1.0  0.565725  0.0  0.0  1.0  0.0       0.0       0.565725\n",
       " 1.0  0.506876  0.0  0.0  0.0  0.0       0.0       0.0     \n",
       " 1.0  0.587189  1.0  0.0  0.0  0.587189  0.0       0.0     "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, X = modelcols(f_concrete, first(df, 6))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting based on new data\n",
    "\n",
    "Any table with the right columns can be passed to `modelcols` and the right\n",
    "columns are generated, even if some levels are missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×8 Array{Float64,2}:\n",
       " 1.0  0.34172   1.0  0.0  0.0  0.34172   0.0       0.0\n",
       " 1.0  0.600846  1.0  0.0  0.0  0.600846  0.0       0.0\n",
       " 1.0  0.308055  0.0  0.0  0.0  0.0       0.0       0.0\n",
       " 1.0  0.275776  0.0  1.0  0.0  0.0       0.275776  0.0\n",
       " 1.0  0.155214  1.0  0.0  0.0  0.155214  0.0       0.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = DataFrame(a = rand(5), b = [:R, :R, :Q, :S, :R])\n",
    "modelcols(f_concrete.rhs, df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a named tuple for a single row\n",
    "\n",
    "A single row of the model matrix can be generated from a `NamedTuple` of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{Float64,1}:\n",
       " 1.0\n",
       " 1.5\n",
       " 0.0\n",
       " 0.0\n",
       " 1.0\n",
       " 0.0\n",
       " 0.0\n",
       " 1.5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_row = (a = 1.5, b = :T)\n",
    "modelcols(f_concrete.rhs, data_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get coefficient names for any term with `coefnames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"y\", [\"(Intercept)\", \"a\", \"b: R\", \"b: S\", \"b: T\", \"a & b: R\", \"a & b: S\", \"a & b: T\"])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefnames(f_concrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{String,1}:\n",
       " \"a & b: R\"\n",
       " \"a & b: S\"\n",
       " \"a & b: T\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefnames(f_concrete.rhs.terms[end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{String,1}:\n",
       " \"b: R\"\n",
       " \"b: S\"\n",
       " \"b: T\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefnames(sch[term(:b)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formula syntax\n",
    "\n",
    "The formula syntax is very similar to R, with the exception that an interaction\n",
    "is specified with `&`, and that some R syntax is not supported by default (`^`,\n",
    "`/` outside of MixedModels.jl).\n",
    "\n",
    "### Non-special calls \n",
    "\n",
    "Any function calls that are not special syntax (`+`, `&`, `*`, and `~`) are\n",
    "treated as normal julia code, so you can write things like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FormulaTerm\n",
       "Response:\n",
       "  (y)->log(y)\n",
       "Predictors:\n",
       "  1\n",
       "  a(unknown)\n",
       "  (a)->a ^ 2\n",
       "  b(unknown)\n",
       "  a(unknown) & b(unknown)\n",
       "  (a)->a ^ 2 & b(unknown)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2 = @formula(log(y) ~ 1 + (a + a^2) * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FormulaTerm\n",
       "Response:\n",
       "  (y)->log(y)\n",
       "Predictors:\n",
       "  1\n",
       "  a(continuous)\n",
       "  (a)->a ^ 2\n",
       "  b(DummyCoding:4→3)\n",
       "  a(continuous) & b(DummyCoding:4→3)\n",
       "  (a)->a ^ 2 & b(DummyCoding:4→3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2_concrete = apply_schema(f2, sch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-0.7523646892478555, -0.05556315511147949, -0.4622129756895239, -1.0951891887927827, -0.2905541141183424], [1.0 0.6504879330034901 … 0.0 0.0; 1.0 0.887735106949233 … 0.0 0.0; … ; 1.0 0.5657251032880737 … 0.0 0.32004489249030166; 1.0 0.5068760737746196 … 0.0 0.0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2, X2 = modelcols(f2_concrete, first(df, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 == log.(df[1:5, :y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×12 Array{Float64,2}:\n",
       " 1.0  0.650488  0.423135  0.0  0.0  0.0  …  0.0       0.0       0.0     \n",
       " 1.0  0.887735  0.788074  1.0  0.0  0.0     0.788074  0.0       0.0     \n",
       " 1.0  0.998381  0.996764  0.0  1.0  0.0     0.0       0.996764  0.0     \n",
       " 1.0  0.565725  0.320045  0.0  0.0  1.0     0.0       0.0       0.320045\n",
       " 1.0  0.506876  0.256923  0.0  0.0  0.0     0.0       0.0       0.0     "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12-element Array{String,1}:\n",
       " \"(Intercept)\" \n",
       " \"a\"           \n",
       " \"a ^ 2\"       \n",
       " \"b: R\"        \n",
       " \"b: S\"        \n",
       " \"b: T\"        \n",
       " \"a & b: R\"    \n",
       " \"a & b: S\"    \n",
       " \"a & b: T\"    \n",
       " \"a ^ 2 & b: R\"\n",
       " \"a ^ 2 & b: S\"\n",
       " \"a ^ 2 & b: T\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefnames(f2_concrete.rhs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced: making the ordinary special\n",
    "\n",
    "You may have noticed that `zercocorr` and `|` were not included in the list of\n",
    "special syntax above.  StatsModels.jl provides a method to add special syntax\n",
    "for the `@formula` that's specific to certain models.  This works using the\n",
    "standard Julia techniques of multiple dispatch, by providing methods that\n",
    "intercept `apply_schema` for particular combinations of functions, schema, and\n",
    "context (model type), like so:\n",
    "\n",
    "```\n",
    "function StatsModels.apply_schema(\n",
    "    t::FunctionTerm{typeof(|)},\n",
    "    schema::StatsModels.FullRank,\n",
    "    Mod::Type{<:MixedModel},\n",
    ")\n",
    "    schema = StatsModels.FullRank(schema.schema)\n",
    "    lhs, rhs = t.args_parsed\n",
    "    if !StatsModels.hasintercept(lhs) && !StatsModels.omitsintercept(lhs)\n",
    "        lhs = InterceptTerm{true}() + lhs\n",
    "    end\n",
    "    lhs, rhs = apply_schema.((lhs, rhs), Ref(schema), Mod)\n",
    "    RandomEffectsTerm(MatrixTerm(lhs), rhs)\n",
    "end\n",
    "```\n",
    "\n",
    "There's a simpler [example in the StatsModels\n",
    "docs](https://juliastats.org/StatsModels.jl/stable/internals/#An-example-of-custom-syntax:-poly-1)\n",
    "which adds a `poly(x, n)` syntax for polynomial regression.\n",
    "\n",
    "### Example: specifying many different models\n",
    "\n",
    "Let's see how each of the predictors in the KB07 dataset does on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{LinearMixedModel{Float64},1}:\n",
       " Linear mixed model fit by maximum likelihood\n",
       " rt_trunc ~ 1 + spkr + (1 | subj)\n",
       "     logLik        -2 logLik          AIC             BIC       \n",
       " -1.47156041×10⁴  2.94312081×10⁴  2.94392081×10⁴  2.94611658×10⁴\n",
       "\n",
       "Variance components:\n",
       "            Column    Variance   Std.Dev. \n",
       "subj     (Intercept)   92268.768 303.75775\n",
       "Residual              777856.310 881.96163\n",
       " Number of obs: 1789; levels of grouping factors: 56\n",
       "\n",
       "  Fixed-effects parameters:\n",
       "──────────────────────────────────────────────────\n",
       "             Estimate  Std.Error  z value  P(>|z|)\n",
       "──────────────────────────────────────────────────\n",
       "(Intercept)  2113.29     50.1677    42.12   <1e-99\n",
       "spkr: old     137.777    41.7046     3.30   0.0010\n",
       "──────────────────────────────────────────────────                  \n",
       " Linear mixed model fit by maximum likelihood\n",
       " rt_trunc ~ 1 + prec + (1 | subj)\n",
       "     logLik        -2 logLik          AIC             BIC       \n",
       " -1.45836279×10⁴  2.91672558×10⁴  2.91752558×10⁴  2.91972134×10⁴\n",
       "\n",
       "Variance components:\n",
       "            Column    Variance   Std.Dev. \n",
       "subj     (Intercept)   95717.665 309.38272\n",
       "Residual              667961.065 817.28885\n",
       " Number of obs: 1789; levels of grouping factors: 56\n",
       "\n",
       "  Fixed-effects parameters:\n",
       "─────────────────────────────────────────────────────\n",
       "                Estimate  Std.Error  z value  P(>|z|)\n",
       "─────────────────────────────────────────────────────\n",
       "(Intercept)     2515.42     49.5539    50.76   <1e-99\n",
       "prec: maintain  -666.945    38.6465   -17.26   <1e-66\n",
       "─────────────────────────────────────────────────────\n",
       " Linear mixed model fit by maximum likelihood\n",
       " rt_trunc ~ 1 + load + (1 | subj)\n",
       "     logLik       -2 logLik         AIC            BIC      \n",
       " -1.4713984×10⁴ 2.94279681×10⁴ 2.94359681×10⁴ 2.94579257×10⁴\n",
       "\n",
       "Variance components:\n",
       "            Column    Variance Std.Dev. \n",
       "subj     (Intercept)   92327.49 303.8544\n",
       "Residual              776400.55 881.1359\n",
       " Number of obs: 1789; levels of grouping factors: 56\n",
       "\n",
       "  Fixed-effects parameters:\n",
       "──────────────────────────────────────────────────\n",
       "             Estimate  Std.Error  z value  P(>|z|)\n",
       "──────────────────────────────────────────────────\n",
       "(Intercept)  2103.56     50.1817    41.92   <1e-99\n",
       "load: yes     156.884    41.6655     3.77   0.0002\n",
       "──────────────────────────────────────────────────                                "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = @formula(rt_trunc ~ 1 + (1|subj) + (1|subj))\n",
    "fits = map([:spkr, :prec, :load]) do p\n",
    "    f = template.lhs ~ template.rhs + term(p)\n",
    "    fit(MixedModel, f, kb07)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which predictor provides the best fit to the data on its own?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29167.0: rt_trunc ~ 1 + prec + (1 | subj)\n",
      "29428.0: rt_trunc ~ 1 + load + (1 | subj)\n",
      "29431.0: rt_trunc ~ 1 + spkr + (1 | subj)\n"
     ]
    }
   ],
   "source": [
    "sort!(fits, by=objective)\n",
    "foreach(fits) do fit\n",
    "    println(round(fit.objective), \": \", fit.formula)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it's `prec`, followed by `load`, and then `spkr`."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "ac1cea8311364b8fb8724b32a6ce63a8",
   "lastKernelId": "8fca0cb7-94c2-4401-adfa-b1f91b7d56ce"
  },
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
