{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A quick tour of StatsModels.jl\n",
    "### Dave Kleinschmidt\n",
    "### 18 February 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, Pipe\n",
    "using MixedModels, GLM\n",
    "using LinearAlgebra, Statistics\n",
    "\n",
    "using StatsModels\n",
    "using StatsModels: pretty_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrast coding\n",
    "\n",
    "## What and why?\n",
    "\n",
    "To fit any kind of statistical model you need some kind of *numerical\n",
    "representation* of your data. Data often comes in a *table*, a named\n",
    "collection of variables of different types of data. Some of that data is\n",
    "\"continuous\", or basically numeric. But often our data is not numeric\n",
    "(or continuous), but \"categorical\", having a finite number of distinct\n",
    "levels.\n",
    "\n",
    "For instance, let's look at the KB07 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb07 = MixedModels.dataset(:kb07)\n",
    "first(kb07, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `:spkr`, `:prec`, and `:load` are categortical variables, each of\n",
    "which takes on two different values. If we fit a regression using this\n",
    "dataset, we end up with predictors that refer to specific levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = @formula(rt_trunc ~ 1 + spkr + prec + spkr&prec + (1 | subj))\n",
    "mod = fit(MixedModel, f, kb07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few rows of the fixed effects design matrix that's\n",
    "generated for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_mat(mod.X[1:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note: all the values are 0 or 1, and there's one column\n",
    "of all 1s at the start (that's the `(Intercept)` term). Columns 2 and 3\n",
    "correspond to `spkr` and `prec`: there's a 0 where `spkr == \"new\"` and a\n",
    "1 for `\"old\"`. Note that the coefficient name for this column is `spkr:\n",
    "old`, which indicates that this predictor indicates the presence of\n",
    "\"old\", relative to the (implicit) baseline of \"new\". Similarly for\n",
    "`prec: maintain`.\n",
    "\n",
    "The last column is the interaction term `spkr&prec`, and it's the\n",
    "elementwise product of the columns for `spkr: new` and `pred: maintain`.\n",
    "\n",
    "## How to take control\n",
    "\n",
    "You can set your own contrasts via the `contrasts=` keyword argument in\n",
    "`fit`, with the variable you want to code as the key and contrasts as\n",
    "the value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using StatsModels\n",
    "\n",
    "contrasts = Dict(\n",
    "    :spkr => EffectsCoding(base = \"old\"),\n",
    "    :prec => DummyCoding(levels = [\"maintain\", \"break\"])\n",
    ")\n",
    "\n",
    "mod2 = fit(MixedModel, f, kb07, contrasts=contrasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example illustrates two ways to control the ordering of levels used\n",
    "to compute the contrasts:\n",
    "\n",
    "1.  you can use `base=` to determine the baseline level\n",
    "2.  you can use `levels=` to indicate all the levels that are used in\n",
    "    the contrasts, the first of which is automatically set as the\n",
    "    baseline.\n",
    "\n",
    "### Reversed Helmert coding\n",
    "\n",
    "Let's say you want to use reverse Helmert coding.  It's easy using `reverse` to\n",
    "flip the order of the levels.  Here's the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spkr_levels = [\"old\",\"new\"]\n",
    "fit(MixedModel,\n",
    "    f,\n",
    "    kb07,\n",
    "    contrasts = Dict(:spkr => HelmertCoding(levels=spkr_levels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And reversed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(MixedModel,\n",
    "    f,\n",
    "    kb07,\n",
    "    contrasts = Dict(:spkr => HelmertCoding(levels=reverse(spkr_levels))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built in contrast coding schemes\n",
    "\n",
    "StatsModels.jl provides a few commonly used contrast coding schemes,\n",
    "some less-commonly used schemes, and structs that allow you to manually\n",
    "specify your own, custom schemes.\n",
    "\n",
    "All are subtypes of the `AbstractContrasts` type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using InteractiveUtils\n",
    "subtypes(AbstractContrasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And all have fairly extensive documentation via the normal help system.\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ?SeqDiffCoding in the REPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard contrasts\n",
    "\n",
    "The most commonly used contrasts are `DummyCoding` and `EffectsCoding`\n",
    "(which are similar to `contr.treatment` and `contr.sum` in R,\n",
    "respectively).\n",
    "\n",
    "### \"Exotic\" contrasts\n",
    "\n",
    "We also provide `HelmertCoding` and `SeqDiffCoding` (corresponding to\n",
    "base R's `contr.helmert` and MASS's `contr.sdiff`).\n",
    "\n",
    "## Manual contrasts\n",
    "\n",
    "There are two ways to manually specify contrasts. First, you can specify\n",
    "them **directly** via `ContrastsCoding`. If you do, it's good practice\n",
    "to specify the levels corresponding to the rows of the matrix, although\n",
    "they can be omitted in which case they'll be inferred from the data.\n",
    "\n",
    "For instance, here's a weird set of contrasts for `:spkr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = Matrix([-1/3 2/3]')\n",
    "contr_manual = Dict(:spkr => StatsModels.ContrastsCoding(cs, levels=[\"old\", \"new\"]))\n",
    "mod3 = fit(MixedModel, f, kb07, contrasts=contr_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that the estimates and even the signs of the fixed effect Î²s\n",
    "change when we change the contrasts, but the overall log-likelihood\n",
    "doesn't).\n",
    "\n",
    "We can see that the values from the contrasts matrix we specified are\n",
    "plugged directly in to the fixed effects matrix, and are also used in\n",
    "computing the predictor for the interaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3.X[1:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: manual Helmert contrasts\n",
    "\n",
    "Let's say you want Helmert contrasts but you always forget what it's called.\n",
    "Here's how you can manually specify them using `StatsModels.ContrastsCoding`.\n",
    "\n",
    "Because this isn't very interesting with only two levels, let's combine `:spkr`\n",
    "and `:prec` into a single, 4-level variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb07ex = transform(kb07, AsTable([:spkr, :prec]) => (x -> x.spkr .* \"-\" .* x.prec) => :spkr_prec);\n",
    "levels = [\"new-break\", \"new-maintain\", \"old-break\", \"old-maintain\"]\n",
    "f2 = @formula(rt_trunc ~ 1 + spkr_prec + (1 | subj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using StatsModels: ContrastsCoding\n",
    "man_helm = [-1 -1 -1\n",
    "             1 -1 -1\n",
    "             0  2 -1\n",
    "             0  0  3]\n",
    "contr_helm_man = ContrastsCoding(man_helm[:,1:3], levels=levels)\n",
    "fit(MixedModel, f2, kb07ex, contrasts = Dict(:spkr_prec => contr_helm_man))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is equivalent to `HelmertCoding`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(MixedModel, f2, kb07ex, contrasts = Dict(:spkr_prec => HelmertCoding(levels=levels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrasts from hypotheses\n",
    "\n",
    "A better way to specify manual contrasts is via `HypothesisCoding`, where each\n",
    "row of the matrix corresponds to the weights given to the cell means of the\n",
    "levels corresponding to each column (see [Schad et\n",
    "al. 2020](https://doi.org/10.1016/j.jml.2019.104038) for more information). As\n",
    "before with manual contrasts, this is less interesting with only two levels, so\n",
    "we'll again look at a scenario where we combine `:spkr` and `:prec` into a\n",
    "single, 4-level predictor, and want to test some strange hypotheses.\n",
    "\n",
    "Here's the model fit with the default (dummy/treatment-coded contrasts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod4 = fit(MixedModel, f2, kb07ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how you could generate custom contrasts for a number of different a\n",
    "priori hypotheses.\n",
    "\n",
    "### Example: Sequential differences coding\n",
    "\n",
    "One hypothesis you might want to test is that the first condition is different\n",
    "from the second, the second from the third, the third from the fourth, etc.\n",
    "First we have to turn these hypotheses into a numeric form.  The null hypothesis\n",
    "that condition 1 is not different from condition 2 can be expressed by saying\n",
    "the difference between the two mean responses in these conditions is zero.\n",
    "Mathematically, we can write that as ``\\mu_2 - \\mu_1 = 0``, or equivalently:\n",
    "\n",
    "```math\n",
    "-1 \\cdot \\mu_1 + 1 \\cdot \\mu_2 + 0 \\cdot \\mu_3 + ... + 0 \\cdot \\mu_n = 0\n",
    "```\n",
    "\n",
    "The weights for each of the means are the entries in our hypothesis vector for\n",
    "this hypothesis.  So the first hypothesis vector is `[-1, 1, 0, 0]`.  Likewise,\n",
    "the second is `[0, -1, 1, 0]` (``\\mu_3 - \\mu_2 = 0``), and the third is `[0, 0,\n",
    "-1, 1]` (``\\mu_4 - \\mu_3 = 0``).  Putting these together we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_diff_hyps = [-1  1  0  0\n",
    "                  0 -1  1  0\n",
    "                  0  0 -1  1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These hypotheses correspond to the following *contrasts* (using the\n",
    "[`StatsModels.pretty_mat`](https://github.com/JuliaStats/StatsModels.jl/blob/master/src/contrasts.jl#L716-L723)\n",
    "function to make pretty fractions; this is based on\n",
    "[`rationalize`](https://docs.julialang.org/en/v1/base/math/#Base.rationalize)\n",
    "function, which is like the `fractions` function in R):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_diff_contrs = HypothesisCoding(seq_diff_hyps, levels = levels)\n",
    "pretty_mat(seq_diff_contrs.contrasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the contrasts for these hypothese are rather different!  It's\n",
    "not immediately obvious just looking at them how they're related (at least not\n",
    "to me), which shows the power of hypothesis coding: you can work in a format\n",
    "that *does* make intuitive sense (the weights assigned to each group's mean\n",
    "response).\n",
    "\n",
    "When we fit the model, we should see that the corresponding betas are the same\n",
    "as the differences between the cell means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(MixedModel,\n",
    "    f2,\n",
    "    kb07ex,\n",
    "    contrasts = Dict(:spkr_prec => seq_diff_contrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the cell mean differences manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_means = @pipe kb07ex |>\n",
    "    groupby(_, :spkr_prec) |>\n",
    "    combine(_, :rt_trunc => mean => :mean_rt) |>\n",
    "    innerjoin(DataFrame(spkr_prec=levels), _, on=:spkr_prec)  # make sure ordering is right\n",
    "diff(cell_means.mean_rt)                                      # compare with betas above..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the intercept corresponds to the *grand mean*, not to the first\n",
    "level's mean!  That's because the hypothesis vectors are *zero-mean*, so they\n",
    "don't affect the hypothesis for the intercept (as long as the design is\n",
    "balanced).\n",
    "\n",
    "### Example: custom, a priori hypotheses\n",
    "\n",
    "Let's say we want to test whether the effect of `:prec` depends on\n",
    "whether `:spkr` is old vs. new. We need one contrast to test the\n",
    "hypothesis that `\"maintain\" != \"break\"` for \"new\", and another for\n",
    "\"old\". That leaves one over, to test the overall difference between\n",
    "\"new\" and \"old\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_old = (levels .== \"old-break\") .- (levels .== \"old-maintain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_new = (levels .== \"new-break\") .- (levels .== \"new-maintain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_new = (abs.(prec_old) .- abs.(prec_new)) ./ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contr_hyp = HypothesisCoding(hcat(old_new, prec_old, prec_new)',\n",
    "                             labels=[\"old\", \"(old) break\", \"(new) break\"])\n",
    "contr_hyp.hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These hypotheses correspond to the following *contrasts*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_mat(contr_hyp.contrasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the Â±1 coding in the hypotheses (which translates into the difference\n",
    "between the mean response in those cells) is transformed into Â±Â½ coding in the\n",
    "contrasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod5 = fit(MixedModel, f2, kb07ex, contrasts = Dict(:spkr_prec => contr_hyp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is equivalent to the `/` \"nesting\" syntax using `EffectsCoding`,\n",
    "after adjusting for the 2Ã factor from the +1/-1 coding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod6 = fit(MixedModel,\n",
    "           @formula(rt_trunc ~ 1 + spkr/prec + (1|subj)), \n",
    "           kb07,\n",
    "           contrasts = Dict(:spkr => EffectsCoding(base=\"new\"),\n",
    "                            :prec => EffectsCoding(base=\"maintain\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Helmert contrasts that actually make sense\n",
    "\n",
    "Let's say you want something like Helmert contrasts, but where the Î²s are\n",
    "interpretable as the difference between the $n$th level and the average of\n",
    "levels $1\\ldots n-1$.  Here are the hypotheses that correspond to that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helmert_hypotheses = [-1 -1/2 -1/3\n",
    "                       1 -1/2 -1/3\n",
    "                       0  1   -1/3\n",
    "                       0  0    1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the resulting contrasts matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contr_helm_hyp = HypothesisCoding(helmert_hypotheses',\n",
    "                                  levels=levels, labels=levels[2:end])\n",
    "pretty_mat(contr_helm_hyp.contrasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is similar but not identical to the contrats matrix for HelmertCoding!  In\n",
    "a way that I would not be able to derive off the top of my head.\n",
    "\n",
    "Now we fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(MixedModel, f2, kb07ex, contrasts = Dict(:spkr_prec => contr_helm_hyp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and we can see that the Î²s for levels are very close to the cumulative means\n",
    "minus the mean for that level, computed manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_means = @pipe kb07ex |>\n",
    "    groupby(_, :spkr_prec) |>\n",
    "    combine(_, :rt_trunc => mean => :mean_rt) |>\n",
    "    innerjoin(DataFrame(spkr_prec=levels), _, on=:spkr_prec) |>\n",
    "    transform(_, :mean_rt => (x -> cumsum(x) ./ (1:length(x))) => :cum_mean) |>\n",
    "    transform(_, [:mean_rt, :cum_mean] => ((x,y) -> x - lag(y)) => :diff_with_last_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The `@formula`\n",
    "\n",
    "A formula in Julia is created with the `@formula` macro.  Between the macro and\n",
    "fitting a model, the formula goes through a number of steps.\n",
    "\n",
    "* The `@formula` macro itself does some transformations on the syntax, and\n",
    "  creates `Term`s\n",
    "* Then a `Schema` is extracted from the data, which says which `Term`s are\n",
    "  `ContinuousTerm`s and which are `CategoricalTerm`s\n",
    "* The `Schema` is then used to transform the original formula into a \"concrete\n",
    "  formula\".\n",
    "* The concrete formula (with all `Term`s replaced by continuous/categorical\n",
    "  versions) generates model matrix columns when given some data.\n",
    "\n",
    "The details are described in the\n",
    "[documentation](https://juliastats.org/StatsModels.jl/stable/internals/#The-lifecycle-of-a-@formula-1),\n",
    "and for the most part modeling packages handle these steps for you.  But in the\n",
    "interest of allowing you to do your own weird things, here are a few examples.\n",
    "\n",
    "## A formula is made of terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = @formula(y ~ 1 + a + b + a&b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inpsect the internal structure with (it's like `str` in R):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build the same formula directly, using terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_a = term(:a)\n",
    "t_b = term(:b)\n",
    "t_1 = term(1)\n",
    "t_y = term(:y)\n",
    "\n",
    "dump(t_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dir = FormulaTerm(t_y, (t_1, t_a, t_b, InteractionTerm((t_a, t_b))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, using operator overloading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_op = t_y ~ t_1 + t_a + t_b + t_a & t_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three are all equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f == f_dir == f_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The schema gives concrete terms\n",
    "\n",
    "If we have some fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(y = rand(100), a = rand(100), b = repeat([:Q, :R, :S, :T], 25))\n",
    "first(df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract a `Schema`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = schema(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This maps un-typed `Term`s to concrete verisons.  Now we know that `:a` is a\n",
    "continuous variable in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch[term(:a)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical terms hold the contrasts matrix and levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_b_concrete = sch[term(:b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = t_b_concrete.contrasts\n",
    "cmat.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat.levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `apply_schema` combines terms and schema to get concrete versions\n",
    "\n",
    "The canonical case is to apply the schema to the whole formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_schema(f, sch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the schema gets pushed through the interaction term, too.\n",
    "\n",
    "We can also apply the schema to a single term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_schema(term(:a) & term(:b), sch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course if the schema doesn't have enough information, we'll get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_schema(term(:argle_bargle), sch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concrete terms generate arrays with `modelcols`\n",
    "\n",
    "Any term can generate model columns with `modelcols`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ab_concrete = apply_schema(term(:a) & term(:b), sch)\n",
    "modelcols(t_ab_concrete, first(df, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_concrete = apply_schema(f, sch)\n",
    "@show t_ab_concrete_formula = f_concrete.rhs.terms[end]\n",
    "modelcols(t_ab_concrete_formula, first(df, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course you can generate columns for the whole formula (it returns a tuple of\n",
    "left-hand side, right-hand side columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = modelcols(f_concrete, first(df, 6))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting based on new data\n",
    "\n",
    "Any table with the right columns can be passed to `modelcols` and the right\n",
    "columns are generated, even if some levels are missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = DataFrame(a = rand(5), b = [:R, :R, :Q, :S, :R])\n",
    "modelcols(f_concrete.rhs, df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a named tuple for a single row\n",
    "\n",
    "A single row of the model matrix can be generated from a `NamedTuple` of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_row = (a = 1.5, b = :T)\n",
    "modelcols(f_concrete.rhs, data_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get coefficient names for any term with `coefnames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefnames(f_concrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefnames(f_concrete.rhs.terms[end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefnames(sch[term(:b)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formula syntax\n",
    "\n",
    "The formula syntax is very similar to R, with the exception that an interaction\n",
    "is specified with `&`, and that some R syntax is not supported by default (`^`,\n",
    "`/` outside of MixedModels.jl).\n",
    "\n",
    "### Non-special calls \n",
    "\n",
    "Any function calls that are not special syntax (`+`, `&`, `*`, and `~`) are\n",
    "treated as normal julia code, so you can write things like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = @formula(log(y) ~ 1 + (a + a^2) * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2_concrete = apply_schema(f2, sch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2, X2 = modelcols(f2_concrete, first(df, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 == log.(df[1:5, :y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefnames(f2_concrete.rhs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced: making the ordinary special\n",
    "\n",
    "You may have noticed that `zercocorr` and `|` were not included in the list of\n",
    "special syntax above.  StatsModels.jl provides a method to add special syntax\n",
    "for the `@formula` that's specific to certain models.  This works using the\n",
    "standard Julia techniques of multiple dispatch, by providing methods that\n",
    "intercept `apply_schema` for particular combinations of functions, schema, and\n",
    "context (model type), like so:\n",
    "\n",
    "```\n",
    "function StatsModels.apply_schema(\n",
    "    t::FunctionTerm{typeof(|)},\n",
    "    schema::StatsModels.FullRank,\n",
    "    Mod::Type{<:MixedModel},\n",
    ")\n",
    "    schema = StatsModels.FullRank(schema.schema)\n",
    "    lhs, rhs = t.args_parsed\n",
    "    if !StatsModels.hasintercept(lhs) && !StatsModels.omitsintercept(lhs)\n",
    "        lhs = InterceptTerm{true}() + lhs\n",
    "    end\n",
    "    lhs, rhs = apply_schema.((lhs, rhs), Ref(schema), Mod)\n",
    "    RandomEffectsTerm(MatrixTerm(lhs), rhs)\n",
    "end\n",
    "```\n",
    "\n",
    "There's a simpler [example in the StatsModels\n",
    "docs](https://juliastats.org/StatsModels.jl/stable/internals/#An-example-of-custom-syntax:-poly-1)\n",
    "which adds a `poly(x, n)` syntax for polynomial regression.\n",
    "\n",
    "### Example: specifying many different models\n",
    "\n",
    "Let's see how each of the predictors in the KB07 dataset does on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = @formula(rt_trunc ~ 1 + (1|subj) + (1|subj))\n",
    "fits = map([:spkr, :prec, :load]) do p\n",
    "    f = template.lhs ~ template.rhs + term(p)\n",
    "    fit(MixedModel, f, kb07)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which predictor provides the best fit to the data on its own?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort!(fits, by=objective)\n",
    "foreach(fits) do fit\n",
    "    println(round(fit.objective), \": \", fit.formula)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it's `prec`, followed by `load`, and then `spkr`."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "fb5e7f76981041aeaf7b399da744b624",
   "lastKernelId": "4f7b9dd2-07c3-40b1-9113-15922a7c9d86"
  },
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
