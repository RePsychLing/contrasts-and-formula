#+TITLE: Contrasts and @formula in StatsModels.jl


* Contrast coding
  :PROPERTIES:
  :header-args: :async yes :kernel julia-1.3 :session jl-contrasts :display text/plain text/html
  :END:

** What and why?

To fit any kind of statistical model you need some kind of /numerical
representation/ of your data.  Data often comes in a /table/, a named collection
of variables of different types of data.  Some of that data is "continuous", or
basically numeric.  But often our data is not numeric (or continuous), but
"categorical", having a finite number of distinct levels.

For instance, let's look at the KB07 dataset:

#+begin_src jupyter-julia
  using DataFrames, DataFramesMeta
  using MixedModels, GLM

  kb07 = MixedModels.dataset(:kb07)

#+end_src

#+RESULTS:
:RESULTS:
: ┌ Info: Precompiling MixedModels [ff71e718-51f3-5ec2-a782-8ffcbfa3c316]
: └ @ Base loading.jl:1273
#+begin_example
  1789×7 DataFrame
  │ Row  │ subj   │ item   │ spkr   │ prec     │ load   │ rt_trunc │ rt_raw │
  │      │ [90mString[39m │ [90mString[39m │ [90mString[39m │ [90mString[39m   │ [90mString[39m │ [90mInt16[39m    │ [90mInt16[39m  │
  ├──────┼────────┼────────┼────────┼──────────┼────────┼──────────┼────────┤
  │ 1    │ S030   │ I01    │ new    │ break    │ yes    │ 2267     │ 2267   │
  │ 2    │ S030   │ I02    │ old    │ maintain │ no     │ 3856     │ 3856   │
  │ 3    │ S030   │ I03    │ old    │ break    │ no     │ 1567     │ 1567   │
  │ 4    │ S030   │ I04    │ new    │ maintain │ no     │ 1732     │ 1732   │
  │ 5    │ S030   │ I05    │ new    │ break    │ no     │ 2660     │ 2660   │
  │ 6    │ S030   │ I06    │ old    │ maintain │ yes    │ 2763     │ 2763   │
  │ 7    │ S030   │ I07    │ old    │ break    │ yes    │ 3528     │ 3528   │
  │ 8    │ S030   │ I08    │ new    │ maintain │ yes    │ 1741     │ 1741   │
  │ 9    │ S030   │ I09    │ new    │ break    │ yes    │ 3692     │ 3692   │
  │ 10   │ S030   │ I10    │ old    │ maintain │ no     │ 1949     │ 1949   │
  ⋮
  │ 1779 │ S103   │ I22    │ new    │ break    │ no     │ 1623     │ 1623   │
  │ 1780 │ S103   │ I23    │ old    │ maintain │ yes    │ 2706     │ 2706   │
  │ 1781 │ S103   │ I24    │ old    │ break    │ yes    │ 4281     │ 4281   │
  │ 1782 │ S103   │ I25    │ new    │ maintain │ yes    │ 2075     │ 2075   │
  │ 1783 │ S103   │ I26    │ new    │ break    │ yes    │ 3179     │ 3179   │
  │ 1784 │ S103   │ I27    │ old    │ maintain │ no     │ 1216     │ 1216   │
  │ 1785 │ S103   │ I28    │ old    │ break    │ no     │ 2286     │ 2286   │
  │ 1786 │ S103   │ I29    │ new    │ maintain │ no     │ 1202     │ 1202   │
  │ 1787 │ S103   │ I30    │ new    │ break    │ no     │ 1581     │ 1581   │
  │ 1788 │ S103   │ I31    │ old    │ maintain │ yes    │ 1601     │ 1601   │
  │ 1789 │ S103   │ I32    │ old    │ break    │ yes    │ 1941     │ 1941   │
#+end_example
:END:

Here ~:spkr~, ~:prec~, and ~:load~ are categortical variables, each of which
takes on two different values.  If we fit a regression using this dataset, we
end up with predictors that refer to specific levels:

#+begin_src jupyter-julia
  f = @formula(rt_trunc ~ 1 + spkr + prec + spkr&prec + (1 | subj))
  mod = fit(MixedModel, f, kb07)
#+end_src

#+RESULTS:
#+begin_example
  Linear mixed model fit by maximum likelihood
   rt_trunc ~ 1 + spkr + prec + spkr & prec + (1 | subj)
       logLik        -2 logLik          AIC             BIC       
   -1.45767208×10⁴  2.91534417×10⁴  2.91654417×10⁴  2.91983781×10⁴

  Variance components:
              Column    Variance  Std.Dev. 
  subj     (Intercept)   95885.39 309.65366
  Residual              662657.47 814.03776
   Number of obs: 1789; levels of grouping factors: 56

    Fixed-effects parameters:
  ──────────────────────────────────────────────────────────────────
                               Estimate  Std.Error  z value  P(>|z|)
  ──────────────────────────────────────────────────────────────────
  (Intercept)                 2425.32      56.5224    42.91   <1e-99
  spkr: old                    179.992     54.4214     3.31   0.0009
  prec: maintain              -623.347     54.4214   -11.45   <1e-29
  spkr: old & prec: maintain   -86.7763    76.9856    -1.13   0.2597
  ──────────────────────────────────────────────────────────────────
#+end_example

Let's look at a few rows of the fixed effects design matrix that's generated for
this model:

#+begin_src jupyter-julia
  Int.(mod.X)[1:5, :]
#+end_src

#+RESULTS:
: 5×4 Array{Int64,2}:
:  1  0  0  0
:  1  1  1  1
:  1  1  0  0
:  1  0  1  0
:  1  0  0  0

A few things to note: all the values are 0 or 1, and there's one column of all
1s at the start (that's the ~(Intercept)~ term).  Columns 2 and 3 correspond to
~spkr~ and ~prec~: there's a 0 where ~spkr == "new"~ and a 1 for ~"old"~.
Note that the coefficient name for this column is ~spkr: old~, which indicates
that this predictor indicates the presence of "old", relative to the (implicit)
baseline of "new".  Similarly for ~prec: maintain~.

The last column is the interaction term ~spkr&prec~, and it's the elementwise
product of the columns for ~spkr: new~ and ~pred: maintain~.

#+begin_src jupyter-julia
  kb07[1:5, :]
#+end_src

#+RESULTS:
: 5×7 DataFrame
: │ Row │ subj   │ item   │ spkr   │ prec     │ load   │ rt_trunc │ rt_raw │
: │     │ [90mString[39m │ [90mString[39m │ [90mString[39m │ [90mString[39m   │ [90mString[39m │ [90mInt16[39m    │ [90mInt16[39m  │
: ├─────┼────────┼────────┼────────┼──────────┼────────┼──────────┼────────┤
: │ 1   │ S030   │ I01    │ new    │ break    │ yes    │ 2267     │ 2267   │
: │ 2   │ S030   │ I02    │ old    │ maintain │ no     │ 3856     │ 3856   │
: │ 3   │ S030   │ I03    │ old    │ break    │ no     │ 1567     │ 1567   │
: │ 4   │ S030   │ I04    │ new    │ maintain │ no     │ 1732     │ 1732   │
: │ 5   │ S030   │ I05    │ new    │ break    │ no     │ 2660     │ 2660   │

** How to take control

You can set your own contrasts via the ~contrasts=~ keyword argument in ~fit~,
with the variable you want to code as the key and contrasts as the value:

#+begin_src jupyter-julia
  using StatsModels

  contrasts = Dict(
      :spkr => EffectsCoding(base = "old"),
      :prec => DummyCoding(levels = ["maintain", "break"])
  )

  mod2 = fit(MixedModel, f, kb07, contrasts=contrasts)
#+end_src

#+RESULTS:
#+begin_example
  Linear mixed model fit by maximum likelihood
   rt_trunc ~ 1 + spkr + prec + spkr & prec + (1 | subj)
       logLik        -2 logLik          AIC             BIC       
   -1.45767208×10⁴  2.91534417×10⁴  2.91654417×10⁴  2.91983781×10⁴

  Variance components:
              Column    Variance  Std.Dev. 
  subj     (Intercept)   95885.39 309.65366
  Residual              662657.47 814.03776
   Number of obs: 1789; levels of grouping factors: 56

    Fixed-effects parameters:
  ───────────────────────────────────────────────────────────────
                            Estimate  Std.Error  z value  P(>|z|)
  ───────────────────────────────────────────────────────────────
  (Intercept)              1848.58      49.5329    37.32   <1e-99
  spkr: new                 -46.6081    27.2263    -1.71   0.0869
  prec: break               666.735     38.4928    17.32   <1e-66
  spkr: new & prec: break   -43.3882    38.4928    -1.13   0.2597
  ───────────────────────────────────────────────────────────────
#+end_example

This example illustrates two ways to control the ordering of levels used to
compute the contrasts: 

1. you can use ~base=~ to determine the baseline level
2. you can use ~levels=~ to indicate all the levels that are used in the
   contrasts, the first of which is automatically set as the baseline.

** Built in contrast coding schemes

StatsModels.jl provides a few commonly used contrast coding schemes, some
less-commonly used schemes, and structs that allow you to manually specify your
own, custom schemes.

All are subtypes of the ~AbstractContrasts~ type:

#+begin_src jupyter-julia
subtypes(AbstractContrasts)
#+end_src

#+RESULTS:
: 7-element Array{Any,1}:
:  ContrastsCoding            
:  DummyCoding                
:  EffectsCoding              
:  HelmertCoding              
:  HypothesisCoding           
:  SeqDiffCoding              
:  StatsModels.FullDummyCoding

And all have fairly extensive documentation via the normal help system.  For
instance:

#+begin_src jupyter-julia
?SeqDiffCoding
#+end_src

#+RESULTS:
:RESULTS:
: search: [0m[1mS[22m[0m[1me[22m[0m[1mq[22m[0m[1mD[22m[0m[1mi[22m[0m[1mf[22m[0m[1mf[22m[0m[1mC[22m[0m[1mo[22m[0m[1md[22m[0m[1mi[22m[0m[1mn[22m[0m[1mg[22m
#+begin_example
  [36m  SeqDiffCoding([base[, levels]])[39m

    Code each level in order to test "sequential difference" hypotheses, which
    compares each level to the level below it (starting with the second level).
    Specifically, the [35mn[39mth predictor tests the hypothesis that the difference
    between levels [35mn[39m and [35mn+1[39m is zero.

  [1m  Examples[22m
  [1m  ≡≡≡≡≡≡≡≡≡≡[22m

  [36m  julia> seqdiff = StatsModels.ContrastsMatrix(SeqDiffCoding(), ["a", "b", "c", "d"]).matrix[39m
  [36m  4×3 Array{Float64,2}:[39m
  [36m   -0.75  -0.5  -0.25[39m
  [36m    0.25  -0.5  -0.25[39m
  [36m    0.25   0.5  -0.25[39m
  [36m    0.25   0.5   0.75[39m

    The interpretation of sequential difference coding may be hard to see from
    the contrasts matrix itself. The corresponding hypothesis matrix shows a
    clearer picture. From the rows of the hypothesis matrix, we can see that
    these contrasts test the difference between the first and second levels, the
    second and third, and the third and fourth, respectively:

  [36m  julia> round.(pinv(seqdiff), digits=2)[39m
  [36m  3×4 Array{Float64,2}:[39m
  [36m   -1.0   1.0  -0.0   0.0[39m
  [36m   -0.0  -1.0   1.0  -0.0[39m
  [36m    0.0  -0.0  -1.0   1.0[39m
#+end_example
:END:

*** Standard contrasts

The most commonly used contrasts are ~DummyCoding~ and ~EffectsCoding~ (which
are similar to ~contr.treatment~ and ~contr.sum~ in R, respectively).

*** "Exotic" contrasts

We also provide ~HelmertCoding~ and ~SeqDiffCoding~ (corresponding to base R's
~contr.helmert~ and MASS's ~contr.sdiff~).
    
*** Manual contrasts

There are two ways to manually specify contrasts.  First, you can specify them
*directly* via ~ContrastsCoding~.  If you do, it's good practice to specify the
levels corresponding to the rows of the matrix, although they can be omitted in
which case they'll be inferred from the data.

For instance, here's a weird set of contrasts for ~:spkr~:

#+begin_src jupyter-julia
cs = Matrix([-1/3 2/3]')
contr_manual = Dict(:spkr => StatsModels.ContrastsCoding(cs, levels=["old", "new"]))
mod3 = fit(MixedModel, f, kb07, contrasts=contr_manual)
#+end_src

#+RESULTS:
:RESULTS:
: ┌ Warning: `ContrastsCoding(contrasts)` is deprecated and will not be exported in the future, use `HypothesisCoding(pinv(contrasts))` instead.
: │   caller = #ContrastsCoding#8 at contrasts.jl:526 [inlined]
: └ @ Core /home/dave/.julia/packages/StatsModels/NakzS/src/contrasts.jl:526
#+begin_example
  Linear mixed model fit by maximum likelihood
   rt_trunc ~ 1 + spkr + prec + spkr & prec + (1 | subj)
       logLik        -2 logLik          AIC             BIC       
   -1.45767208×10⁴  2.91534417×10⁴  2.91654417×10⁴  2.91983781×10⁴

  Variance components:
              Column    Variance  Std.Dev. 
  subj     (Intercept)   95885.39 309.65366
  Residual              662657.47 814.03776
   Number of obs: 1789; levels of grouping factors: 56

    Fixed-effects parameters:
  ──────────────────────────────────────────────────────────────────
                               Estimate  Std.Error  z value  P(>|z|)
  ──────────────────────────────────────────────────────────────────
  (Intercept)                 2545.31      50.3425    50.56   <1e-99
  spkr: new                   -179.992     54.4214    -3.31   0.0009
  prec: maintain              -681.198     40.582    -16.79   <1e-62
  spkr: new & prec: maintain    86.7763    76.9856     1.13   0.2597
  ──────────────────────────────────────────────────────────────────
#+end_example
:END:

(Note that the estimates and even the signs of the fixed effect βs change when
we change the contrasts, but the overall log-likelihood doesn't).

We can see that the values from the contrasts matrix we specified are plugged
directly in to the fixed effects matrix, and are also used in computing the
predictor for the interaction:

#+begin_src jupyter-julia
mod3.X[1:5, :]
#+end_src

#+RESULTS:
: 5×4 Array{Float64,2}:
:  1.0   0.666667  0.0   0.0     
:  1.0  -0.333333  1.0  -0.333333
:  1.0  -0.333333  0.0  -0.0     
:  1.0   0.666667  1.0   0.666667
:  1.0   0.666667  0.0   0.0     

*** Contrasts from hypotheses

A better way to specify manual contrasts is via ~HypothesisCoding~, where each
row of the matrix corresponds to the weights given to the cell means of the
levels corresponding to each column (see [[https://doi.org/10.1016/j.jml.2019.104038][Schad et al. 2020]] for more
information).  This is less interesting with only two levels, so let's look at a
scenario where we combine ~:spkr~ and ~:prec~ into a single, 4-level predictor,
and want to test some strange hypotheses.

#+begin_src jupyter-julia
  kb07ex = @transform(kb07, spkr_prec = :spkr .* "-" .* :prec);

  f2 = @formula(rt_trunc ~ 1 + spkr_prec + (1 | subj))
  mod4 = fit(MixedModel, f2, kb07ex)
#+end_src

#+RESULTS:
#+begin_example
  Linear mixed model fit by maximum likelihood
   rt_trunc ~ 1 + spkr_prec + (1 | subj)
       logLik        -2 logLik          AIC             BIC       
   -1.45767208×10⁴  2.91534417×10⁴  2.91654417×10⁴  2.91983781×10⁴

  Variance components:
              Column    Variance  Std.Dev. 
  subj     (Intercept)   95885.39 309.65366
  Residual              662657.47 814.03776
   Number of obs: 1789; levels of grouping factors: 56

    Fixed-effects parameters:
  ──────────────────────────────────────────────────────────────
                           Estimate  Std.Error  z value  P(>|z|)
  ──────────────────────────────────────────────────────────────
  (Intercept)              2425.32     56.5224    42.91   <1e-99
  spkr_prec: new-maintain  -623.347    54.4214   -11.45   <1e-29
  spkr_prec: old-break      179.992    54.4214     3.31   0.0009
  spkr_prec: old-maintain  -530.131    54.4839    -9.73   <1e-21
  ──────────────────────────────────────────────────────────────
#+end_example

Let's say we want to test whether the effect of ~:prec~ depends on whether
~:spkr~ is old vs. new.  We need one contrast to test the hypothesis that
~"maintain" != "break"~ for "new", and another for "old".  That leaves one over,
to test the overall difference between "new" and "old".

#+begin_src jupyter-julia
  levels = ["new-break", "new-maintain", "old-break", "old-maintain"]

  prec_old = (levels .== "old-break") .- (levels .== "old-maintain")
  prec_new = (levels .== "new-break") .- (levels .== "new-maintain")
  old_new = (abs.(prec_old) .- abs.(prec_new)) ./ 2

  hypotheses = Matrix(hcat(old_new, prec_old, prec_new)')
#+end_src

#+RESULTS:
: 3×4 Array{Float64,2}:
:  -0.5  -0.5  0.5   0.5
:   0.0   0.0  1.0  -1.0
:   1.0  -1.0  0.0   0.0

#+begin_src jupyter-julia
  contr_hyp = Dict(:spkr_prec => HypothesisCoding(hypotheses,
                                                  labels=["old", "(old) break", "(old) maint"]))
  mod5 = fit(MixedModel, f2, kb07ex, contrasts = contr_hyp)
#+end_src

#+RESULTS:
#+begin_example
  Linear mixed model fit by maximum likelihood
   rt_trunc ~ 1 + spkr_prec + (1 | subj)
       logLik        -2 logLik          AIC             BIC       
   -1.45767208×10⁴  2.91534417×10⁴  2.91654417×10⁴  2.91983781×10⁴

  Variance components:
              Column    Variance  Std.Dev. 
  subj     (Intercept)   95885.39 309.65366
  Residual              662657.47 814.03776
   Number of obs: 1789; levels of grouping factors: 56

    Fixed-effects parameters:
  ─────────────────────────────────────────────────────────────
                          Estimate  Std.Error  z value  P(>|z|)
  ─────────────────────────────────────────────────────────────
  (Intercept)             2181.94     45.6362    47.81   <1e-99
  spkr_prec: old           136.604    38.4928     3.55   0.0004
  spkr_prec: (old) break   710.123    54.4527    13.04   <1e-38
  spkr_prec: (old) maint   623.347    54.4214    11.45   <1e-29
  ─────────────────────────────────────────────────────────────
#+end_example
